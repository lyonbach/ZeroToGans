{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link To The Video:\n",
    "#### https://www.youtube.com/watch?v=hvLFD4AZzCw&list=PLyMom0n-MBroupZiLfVSZqK5asX8KfoHL&index=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex     bmi  children smoker     region      charges\n",
       "0   19  female  27.900         0    yes  southwest  16884.92400\n",
       "1   18    male  33.770         1     no  southeast   1725.55230\n",
       "2   28    male  33.000         3     no  southeast   4449.46200\n",
       "3   33    male  22.705         0     no  northwest  21984.47061\n",
       "4   32    male  28.880         0     no  northwest   3866.85520"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv(Path(\"../../data/KaggleMedicalCostPersonalDataset/insureance.csv\"))\n",
    "dataframe.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: How many rows does the dataframe have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1338"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To determine the row count we can use the shape attribute.\n",
    "dataframe.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What are the column titles of output/target variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       16884.92400\n",
       "1        1725.55230\n",
       "2        4449.46200\n",
       "3       21984.47061\n",
       "4        3866.85520\n",
       "           ...     \n",
       "1333    10600.54830\n",
       "1334     2205.98080\n",
       "1335     1629.83350\n",
       "1336     2007.94500\n",
       "1337    29141.36030\n",
       "Name: charges, Length: 1338, dtype: float64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.charges"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Which of the input columns are non-numeric or categorical variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following columns contain non numeric values:\n",
      "sex\n",
      "smoker\n",
      "region\n"
     ]
    }
   ],
   "source": [
    "non_numeric = []\n",
    "for column in dataframe.columns:\n",
    "    converted = pd.to_numeric(dataframe[column], errors=\"coerce\")\n",
    "    if converted.isnull().any():\n",
    "        non_numeric.append(column)\n",
    "\n",
    "print(\"Following columns contain non numeric values:\")\n",
    "for item in non_numeric:\n",
    "    print(item)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What are the minimum, maximum and average values of the charges column? Can you show the distrubution of the values in a graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 63770.4280, Min: 1121.8739, Average: 13270.4223\n"
     ]
    }
   ],
   "source": [
    "charges_min = dataframe[\"charges\"].min()\n",
    "charges_max = dataframe[\"charges\"].max()\n",
    "charges_mean = dataframe[\"charges\"].mean()\n",
    "print(f\"Max: {charges_max:.4f}, Min: {charges_min:.4f}, Average: {charges_mean:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_columns(dataframe):\n",
    "\n",
    "    input_columns = dataframe.columns.tolist()\n",
    "    input_columns.remove(\"charges\")\n",
    "    return input_columns\n",
    "\n",
    "def get_output_columns(dataframe):\n",
    "\n",
    "    return [\"charges\"]\n",
    "\n",
    "def get_categorical_columns(dataframe):\n",
    "\n",
    "    categorical = []\n",
    "    for column in dataframe.columns:\n",
    "        converted = pd.to_numeric(dataframe[column], errors=\"coerce\")\n",
    "        if converted.isnull().any():\n",
    "            categorical.append(column)\n",
    "\n",
    "    return categorical\n",
    "\n",
    "def dataframe_to_arrays(dataframe):\n",
    "\n",
    "    # Make a copy of the original dataframe\n",
    "    dataframe_ = dataframe.copy(deep=True)\n",
    "\n",
    "    # Convert non-numeric categorical columns to numbers\n",
    "    for col in get_categorical_columns(dataframe):\n",
    "        dataframe_[col] = dataframe_[col].astype(\"category\").cat.codes\n",
    "\n",
    "    # Extract input & outputs as numpy arrays\n",
    "    inputs_array = dataframe_[get_input_columns(dataframe)].to_numpy()\n",
    "    targets_array = dataframe_[get_output_columns(dataframe)].to_numpy()\n",
    "    return inputs_array, targets_array"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Convert the numpy arrays input_array and targets_array to PyTorch tensors. Make sure that the data type is float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "inputs_array, targets_array = dataframe_to_arrays(dataframe)\n",
    "inputs_tensors = torch.tensor(inputs_array).to(torch.float32)\n",
    "targets_tensors = torch.tensor(targets_array).to(torch.float32)\n",
    "\n",
    "print(inputs_tensors.dtype)\n",
    "print(targets_tensors.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(inputs_tensors, targets_tensors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Pick a number between 0.1 and 0.2 to determine the fraction of data that will be used for creating the validation set.\n",
    "Then use random_split to create training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_percent = .15\n",
    "row_count = dataframe.shape[0]\n",
    "validation_size = int(row_count * validation_percent)\n",
    "train_size = row_count - validation_size\n",
    "train_dataset, validation_dataset =  random_split(dataset, [train_size, validation_size])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Pick a batch size for the data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsuranceModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "\n",
    "        super().__init__()\n",
    "        hidden_size = 500\n",
    "        self.layer_0 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer_1 = nn.LazyLinear(hidden_size, output_size)\n",
    "        self.loss_fn = F.mse_loss\n",
    "\n",
    "    def forward(self, xb):\n",
    "\n",
    "        return self.layer_1(self.layer_0(xb))\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "\n",
    "        inputs, targets = batch\n",
    "\n",
    "        # Generate predictions\n",
    "        outputs = self(inputs)\n",
    "\n",
    "        # Calculate loss \n",
    "        return self.loss_fn(outputs, targets)\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "\n",
    "        inputs, targets = batch\n",
    "    \n",
    "        # Generate predictions\n",
    "        outputs = self(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = self.loss_fn(outputs, targets)\n",
    "\n",
    "        return {\"val_loss\": loss.detach()}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        batch_losses = [x[\"val_loss\"] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        return {\"val_loss\": epoch_loss.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result, num_epochs):\n",
    "\n",
    "        if (epoch + 1) % 20 == 0 or epoch == num_epochs - 1:\n",
    "            print(f\"Epoch [{epoch + 1}], loss: {result['val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, validation_loader):\n",
    "\n",
    "    outputs = [model.validation_step(batch) for batch in validation_loader]\n",
    "    return model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, lr, model, train_loader, validation_loader, optimizer_function=torch.optim.Adam):\n",
    "\n",
    "    history = []\n",
    "    optimizer = optimizer_function(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_loader:  # Training phase\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        result = evaluate(model, validation_loader)  # Validation phase\n",
    "        model.epoch_end(epoch, result, epochs)\n",
    "        history.append(result)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/lyonbach/depot/Work/Projects/Courses/YoutubeZeroToGans/interpreter/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "input_size = len(get_input_columns(dataframe))\n",
    "output_size = len(get_output_columns(dataframe))\n",
    "model = InsuranceModel(input_size, output_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Use the evaluate function to calculate the loss on the validation set before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14744/275731345.py:33: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 500])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = self.loss_fn(outputs, targets)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 285007648.0}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14744/275731345.py:23: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 500])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return self.loss_fn(outputs, targets)\n",
      "/tmp/ipykernel_14744/275731345.py:23: UserWarning: Using a target size (torch.Size([38, 1])) that is different to the input size (torch.Size([38, 500])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return self.loss_fn(outputs, targets)\n",
      "/tmp/ipykernel_14744/275731345.py:33: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 500])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = self.loss_fn(outputs, targets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], loss: 111936800.0000\n",
      "Epoch [40], loss: 111022696.0000\n",
      "Epoch [60], loss: 110089392.0000\n",
      "Epoch [80], loss: 109043440.0000\n",
      "Epoch [100], loss: 107932904.0000\n",
      "Epoch [120], loss: 106796648.0000\n",
      "Epoch [140], loss: 105327504.0000\n",
      "Epoch [160], loss: 103816872.0000\n",
      "Epoch [180], loss: 101847368.0000\n",
      "Epoch [200], loss: 99330944.0000\n",
      "Epoch [220], loss: 96975456.0000\n",
      "Epoch [240], loss: 93206968.0000\n",
      "Epoch [260], loss: 89231216.0000\n",
      "Epoch [280], loss: 84377736.0000\n",
      "Epoch [300], loss: 78945760.0000\n",
      "Epoch [320], loss: 72811248.0000\n",
      "Epoch [340], loss: 66051500.0000\n",
      "Epoch [360], loss: 59698152.0000\n",
      "Epoch [380], loss: 53000060.0000\n",
      "Epoch [400], loss: 47010264.0000\n",
      "Epoch [420], loss: 41949072.0000\n",
      "Epoch [440], loss: 37678680.0000\n",
      "Epoch [460], loss: 34327532.0000\n",
      "Epoch [480], loss: 32335604.0000\n",
      "Epoch [500], loss: 30911252.0000\n",
      "Epoch [520], loss: 30147074.0000\n",
      "Epoch [540], loss: 30094274.0000\n",
      "Epoch [560], loss: 30236966.0000\n",
      "Epoch [580], loss: 30471116.0000\n",
      "Epoch [600], loss: 30398932.0000\n",
      "Epoch [620], loss: 30512924.0000\n",
      "Epoch [640], loss: 30084152.0000\n",
      "Epoch [660], loss: 29861416.0000\n",
      "Epoch [680], loss: 30032768.0000\n",
      "Epoch [700], loss: 29749020.0000\n",
      "Epoch [720], loss: 29785348.0000\n",
      "Epoch [740], loss: 29552980.0000\n",
      "Epoch [760], loss: 29595292.0000\n",
      "Epoch [780], loss: 29419198.0000\n",
      "Epoch [800], loss: 29626770.0000\n",
      "Epoch [820], loss: 29473188.0000\n",
      "Epoch [840], loss: 29193984.0000\n",
      "Epoch [860], loss: 29344728.0000\n",
      "Epoch [880], loss: 29195672.0000\n",
      "Epoch [900], loss: 29270382.0000\n",
      "Epoch [920], loss: 29135000.0000\n",
      "Epoch [940], loss: 29110264.0000\n",
      "Epoch [960], loss: 28844854.0000\n",
      "Epoch [980], loss: 28793868.0000\n",
      "Epoch [1000], loss: 28883646.0000\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "lr = 1e-3\n",
    "history_0 = fit(epochs, lr, model, train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14744/275731345.py:23: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 500])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return self.loss_fn(outputs, targets)\n",
      "/tmp/ipykernel_14744/275731345.py:23: UserWarning: Using a target size (torch.Size([38, 1])) that is different to the input size (torch.Size([38, 500])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return self.loss_fn(outputs, targets)\n",
      "/tmp/ipykernel_14744/275731345.py:33: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50, 500])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = self.loss_fn(outputs, targets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], loss: 27395462.0000\n",
      "Epoch [40], loss: 27600402.0000\n",
      "Epoch [60], loss: 27385686.0000\n",
      "Epoch [80], loss: 27460312.0000\n",
      "Epoch [100], loss: 27263952.0000\n",
      "Epoch [120], loss: 32340790.0000\n",
      "Epoch [140], loss: 27747750.0000\n",
      "Epoch [160], loss: 28055952.0000\n",
      "Epoch [180], loss: 27660048.0000\n",
      "Epoch [200], loss: 31551768.0000\n",
      "Epoch [220], loss: 29725748.0000\n",
      "Epoch [240], loss: 27325352.0000\n",
      "Epoch [260], loss: 27402376.0000\n",
      "Epoch [280], loss: 27344504.0000\n",
      "Epoch [300], loss: 30993736.0000\n",
      "Epoch [320], loss: 27610356.0000\n",
      "Epoch [340], loss: 27846370.0000\n",
      "Epoch [360], loss: 29028196.0000\n",
      "Epoch [380], loss: 27299148.0000\n",
      "Epoch [400], loss: 28831972.0000\n",
      "Epoch [420], loss: 29933828.0000\n",
      "Epoch [440], loss: 30219702.0000\n",
      "Epoch [460], loss: 29288690.0000\n",
      "Epoch [480], loss: 28107308.0000\n",
      "Epoch [500], loss: 28962622.0000\n",
      "Epoch [520], loss: 27816032.0000\n",
      "Epoch [540], loss: 27407340.0000\n",
      "Epoch [560], loss: 29953092.0000\n",
      "Epoch [580], loss: 27797814.0000\n",
      "Epoch [600], loss: 27424956.0000\n",
      "Epoch [620], loss: 29813204.0000\n",
      "Epoch [640], loss: 28417472.0000\n",
      "Epoch [660], loss: 27785592.0000\n",
      "Epoch [680], loss: 27360168.0000\n",
      "Epoch [700], loss: 30556648.0000\n",
      "Epoch [720], loss: 31608376.0000\n",
      "Epoch [740], loss: 27471426.0000\n",
      "Epoch [760], loss: 27502854.0000\n",
      "Epoch [780], loss: 27888496.0000\n",
      "Epoch [800], loss: 28081672.0000\n",
      "Epoch [820], loss: 27353936.0000\n",
      "Epoch [840], loss: 29632070.0000\n",
      "Epoch [860], loss: 27990264.0000\n",
      "Epoch [880], loss: 27968104.0000\n",
      "Epoch [900], loss: 27815240.0000\n",
      "Epoch [920], loss: 27740882.0000\n",
      "Epoch [940], loss: 27947598.0000\n",
      "Epoch [960], loss: 27277974.0000\n",
      "Epoch [980], loss: 28224484.0000\n",
      "Epoch [1000], loss: 29123806.0000\n",
      "Epoch [1020], loss: 28657368.0000\n",
      "Epoch [1040], loss: 28411970.0000\n",
      "Epoch [1060], loss: 27981736.0000\n",
      "Epoch [1080], loss: 27311040.0000\n",
      "Epoch [1100], loss: 28635956.0000\n",
      "Epoch [1120], loss: 29579624.0000\n",
      "Epoch [1140], loss: 27504194.0000\n",
      "Epoch [1160], loss: 32735306.0000\n",
      "Epoch [1180], loss: 27281358.0000\n",
      "Epoch [1200], loss: 27668832.0000\n",
      "Epoch [1220], loss: 28398228.0000\n",
      "Epoch [1240], loss: 27287374.0000\n",
      "Epoch [1260], loss: 29304492.0000\n",
      "Epoch [1280], loss: 28314194.0000\n",
      "Epoch [1300], loss: 27270474.0000\n",
      "Epoch [1320], loss: 29293426.0000\n",
      "Epoch [1340], loss: 28031332.0000\n",
      "Epoch [1360], loss: 27620940.0000\n",
      "Epoch [1380], loss: 27418340.0000\n",
      "Epoch [1400], loss: 27831016.0000\n",
      "Epoch [1420], loss: 27720504.0000\n",
      "Epoch [1440], loss: 27326250.0000\n",
      "Epoch [1460], loss: 29219862.0000\n",
      "Epoch [1480], loss: 30446740.0000\n",
      "Epoch [1500], loss: 27431416.0000\n",
      "Epoch [1520], loss: 28059984.0000\n",
      "Epoch [1540], loss: 31383884.0000\n",
      "Epoch [1560], loss: 27312160.0000\n",
      "Epoch [1580], loss: 28102354.0000\n",
      "Epoch [1600], loss: 28209352.0000\n",
      "Epoch [1620], loss: 27407986.0000\n",
      "Epoch [1640], loss: 29659624.0000\n",
      "Epoch [1660], loss: 28562988.0000\n",
      "Epoch [1680], loss: 29643852.0000\n",
      "Epoch [1700], loss: 28186424.0000\n",
      "Epoch [1720], loss: 28313316.0000\n",
      "Epoch [1740], loss: 28610898.0000\n",
      "Epoch [1760], loss: 27958740.0000\n",
      "Epoch [1780], loss: 27750156.0000\n",
      "Epoch [1800], loss: 27877776.0000\n",
      "Epoch [1820], loss: 27456812.0000\n",
      "Epoch [1840], loss: 28532250.0000\n",
      "Epoch [1860], loss: 27614808.0000\n",
      "Epoch [1880], loss: 28594704.0000\n",
      "Epoch [1900], loss: 28681998.0000\n",
      "Epoch [1920], loss: 30626396.0000\n",
      "Epoch [1940], loss: 27734480.0000\n",
      "Epoch [1960], loss: 28428600.0000\n",
      "Epoch [1980], loss: 28411244.0000\n",
      "Epoch [2000], loss: 27535794.0000\n",
      "Epoch [2020], loss: 29298822.0000\n",
      "Epoch [2040], loss: 29962972.0000\n",
      "Epoch [2060], loss: 31045844.0000\n",
      "Epoch [2080], loss: 27940084.0000\n",
      "Epoch [2100], loss: 27574766.0000\n",
      "Epoch [2120], loss: 27338200.0000\n",
      "Epoch [2140], loss: 28148180.0000\n",
      "Epoch [2160], loss: 27755982.0000\n",
      "Epoch [2180], loss: 28779502.0000\n",
      "Epoch [2200], loss: 27669700.0000\n",
      "Epoch [2220], loss: 28152946.0000\n",
      "Epoch [2240], loss: 27296884.0000\n",
      "Epoch [2260], loss: 27307064.0000\n",
      "Epoch [2280], loss: 28881028.0000\n",
      "Epoch [2300], loss: 27772596.0000\n",
      "Epoch [2320], loss: 29131148.0000\n",
      "Epoch [2340], loss: 27425896.0000\n",
      "Epoch [2360], loss: 28405788.0000\n",
      "Epoch [2380], loss: 27250360.0000\n",
      "Epoch [2400], loss: 28667320.0000\n",
      "Epoch [2420], loss: 29413426.0000\n",
      "Epoch [2440], loss: 27814560.0000\n",
      "Epoch [2460], loss: 30798400.0000\n",
      "Epoch [2480], loss: 29401784.0000\n",
      "Epoch [2500], loss: 31831856.0000\n",
      "Epoch [2520], loss: 27235672.0000\n",
      "Epoch [2540], loss: 27291250.0000\n",
      "Epoch [2560], loss: 27489648.0000\n",
      "Epoch [2580], loss: 27323466.0000\n",
      "Epoch [2600], loss: 28516790.0000\n",
      "Epoch [2620], loss: 27637972.0000\n",
      "Epoch [2640], loss: 27553336.0000\n",
      "Epoch [2660], loss: 27371618.0000\n",
      "Epoch [2680], loss: 28146208.0000\n",
      "Epoch [2700], loss: 27629180.0000\n",
      "Epoch [2720], loss: 27883966.0000\n",
      "Epoch [2740], loss: 27812382.0000\n",
      "Epoch [2760], loss: 27271384.0000\n",
      "Epoch [2780], loss: 27185992.0000\n",
      "Epoch [2800], loss: 32122804.0000\n",
      "Epoch [2820], loss: 29521960.0000\n",
      "Epoch [2840], loss: 27862384.0000\n",
      "Epoch [2860], loss: 27371144.0000\n",
      "Epoch [2880], loss: 27859730.0000\n",
      "Epoch [2900], loss: 27681800.0000\n",
      "Epoch [2920], loss: 28306856.0000\n",
      "Epoch [2940], loss: 27895904.0000\n",
      "Epoch [2960], loss: 27435070.0000\n",
      "Epoch [2980], loss: 27826876.0000\n",
      "Epoch [3000], loss: 28477140.0000\n",
      "Epoch [3020], loss: 29010800.0000\n",
      "Epoch [3040], loss: 28359378.0000\n",
      "Epoch [3060], loss: 27627796.0000\n",
      "Epoch [3080], loss: 28458518.0000\n",
      "Epoch [3100], loss: 30426606.0000\n",
      "Epoch [3120], loss: 30257308.0000\n",
      "Epoch [3140], loss: 31157434.0000\n",
      "Epoch [3160], loss: 29309244.0000\n",
      "Epoch [3180], loss: 28513198.0000\n",
      "Epoch [3200], loss: 27460096.0000\n",
      "Epoch [3220], loss: 27450282.0000\n",
      "Epoch [3240], loss: 27227066.0000\n",
      "Epoch [3260], loss: 27732756.0000\n",
      "Epoch [3280], loss: 27987056.0000\n",
      "Epoch [3300], loss: 29169816.0000\n",
      "Epoch [3320], loss: 27921730.0000\n",
      "Epoch [3340], loss: 28408986.0000\n",
      "Epoch [3360], loss: 27421276.0000\n",
      "Epoch [3380], loss: 28350820.0000\n",
      "Epoch [3400], loss: 27917932.0000\n",
      "Epoch [3420], loss: 27445330.0000\n",
      "Epoch [3440], loss: 27554142.0000\n",
      "Epoch [3460], loss: 28133076.0000\n",
      "Epoch [3480], loss: 28280788.0000\n",
      "Epoch [3500], loss: 27285334.0000\n",
      "Epoch [3520], loss: 27865168.0000\n",
      "Epoch [3540], loss: 30054786.0000\n",
      "Epoch [3560], loss: 29010246.0000\n",
      "Epoch [3580], loss: 28772120.0000\n",
      "Epoch [3600], loss: 27290368.0000\n",
      "Epoch [3620], loss: 28674246.0000\n",
      "Epoch [3640], loss: 28212828.0000\n",
      "Epoch [3660], loss: 27765820.0000\n",
      "Epoch [3680], loss: 27370230.0000\n",
      "Epoch [3700], loss: 27285164.0000\n",
      "Epoch [3720], loss: 27363304.0000\n",
      "Epoch [3740], loss: 27469562.0000\n",
      "Epoch [3760], loss: 27912606.0000\n",
      "Epoch [3780], loss: 30090648.0000\n",
      "Epoch [3800], loss: 27350190.0000\n",
      "Epoch [3820], loss: 27482554.0000\n",
      "Epoch [3840], loss: 28167296.0000\n",
      "Epoch [3860], loss: 29502082.0000\n",
      "Epoch [3880], loss: 28368264.0000\n",
      "Epoch [3900], loss: 28908630.0000\n",
      "Epoch [3920], loss: 27308044.0000\n",
      "Epoch [3940], loss: 28237130.0000\n",
      "Epoch [3960], loss: 27212384.0000\n",
      "Epoch [3980], loss: 27629606.0000\n",
      "Epoch [4000], loss: 27412068.0000\n",
      "Epoch [4020], loss: 30982148.0000\n",
      "Epoch [4040], loss: 28669320.0000\n",
      "Epoch [4060], loss: 28400320.0000\n",
      "Epoch [4080], loss: 27748168.0000\n",
      "Epoch [4100], loss: 27354986.0000\n",
      "Epoch [4120], loss: 28789818.0000\n",
      "Epoch [4140], loss: 28421732.0000\n",
      "Epoch [4160], loss: 28853556.0000\n",
      "Epoch [4180], loss: 27508564.0000\n",
      "Epoch [4200], loss: 27404568.0000\n",
      "Epoch [4220], loss: 28867478.0000\n",
      "Epoch [4240], loss: 27290736.0000\n",
      "Epoch [4260], loss: 27368758.0000\n",
      "Epoch [4280], loss: 28544538.0000\n",
      "Epoch [4300], loss: 29163786.0000\n",
      "Epoch [4320], loss: 27648552.0000\n",
      "Epoch [4340], loss: 27288892.0000\n",
      "Epoch [4360], loss: 29082684.0000\n",
      "Epoch [4380], loss: 27659576.0000\n",
      "Epoch [4400], loss: 28978076.0000\n",
      "Epoch [4420], loss: 27565432.0000\n",
      "Epoch [4440], loss: 27694740.0000\n",
      "Epoch [4460], loss: 27406056.0000\n",
      "Epoch [4480], loss: 27822040.0000\n",
      "Epoch [4500], loss: 27882688.0000\n",
      "Epoch [4520], loss: 27715142.0000\n",
      "Epoch [4540], loss: 28772364.0000\n",
      "Epoch [4560], loss: 31141074.0000\n",
      "Epoch [4580], loss: 30434298.0000\n",
      "Epoch [4600], loss: 28097384.0000\n",
      "Epoch [4620], loss: 29339580.0000\n",
      "Epoch [4640], loss: 29819374.0000\n",
      "Epoch [4660], loss: 30331836.0000\n",
      "Epoch [4680], loss: 27272628.0000\n",
      "Epoch [4700], loss: 28117412.0000\n",
      "Epoch [4720], loss: 27312770.0000\n",
      "Epoch [4740], loss: 27289344.0000\n",
      "Epoch [4760], loss: 27525498.0000\n",
      "Epoch [4780], loss: 29350828.0000\n",
      "Epoch [4800], loss: 27862100.0000\n",
      "Epoch [4820], loss: 28826080.0000\n",
      "Epoch [4840], loss: 27587982.0000\n",
      "Epoch [4860], loss: 27436496.0000\n",
      "Epoch [4880], loss: 27288704.0000\n",
      "Epoch [4900], loss: 28377430.0000\n",
      "Epoch [4920], loss: 27277100.0000\n",
      "Epoch [4940], loss: 27691670.0000\n",
      "Epoch [4960], loss: 28015942.0000\n",
      "Epoch [4980], loss: 28336476.0000\n",
      "Epoch [5000], loss: 31678132.0000\n"
     ]
    }
   ],
   "source": [
    "epochs = 5000\n",
    "lr = 1e-2\n",
    "history_0 = fit(epochs, lr, model, train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1115.2299, -1114.8939, -1114.8011, -1114.8809, -1115.4144, -1115.3270,\n",
      "        -1114.9404, -1114.4325, -1114.8660, -1114.9890, -1115.0845, -1114.5745,\n",
      "        -1115.0720, -1114.8093, -1114.6744, -1114.7313, -1114.8679, -1115.3007,\n",
      "        -1115.1625, -1115.2877, -1115.1005, -1114.7206, -1114.9952, -1114.8074,\n",
      "        -1114.8568, -1114.8214, -1115.1595, -1114.5918, -1115.0573, -1115.3767,\n",
      "        -1115.3141, -1115.2604, -1114.7008, -1114.8917, -1115.2141, -1114.8132,\n",
      "        -1114.8922, -1114.8555, -1114.7000, -1114.7167, -1115.2335, -1115.1301,\n",
      "        -1114.7477, -1114.9037, -1114.8739, -1115.0635, -1115.0669, -1114.9233,\n",
      "        -1115.1940, -1114.8877, -1114.8434, -1115.1498, -1115.1794, -1115.1062,\n",
      "        -1115.1238, -1114.8584, -1115.2111, -1114.8147, -1115.1486, -1114.8746,\n",
      "        -1114.7935, -1115.4674, -1115.3602, -1114.8154, -1115.0858, -1114.6051,\n",
      "        -1114.5927, -1114.3783, -1114.7483, -1115.0175, -1114.9987, -1115.2153,\n",
      "        -1114.9324, -1115.1801, -1114.8171, -1115.3179, -1115.2426, -1115.0898,\n",
      "        -1114.9855, -1115.0887, -1114.7776, -1115.1569, -1114.7913, -1114.9861,\n",
      "        -1114.9252, -1114.7316, -1114.7148, -1115.2205, -1115.0154, -1115.3002,\n",
      "        -1114.8342, -1115.0037, -1115.3265, -1115.0964, -1114.6293, -1115.0735,\n",
      "        -1114.8823, -1115.2799, -1114.6835, -1115.4276, -1115.5184, -1115.0682,\n",
      "        -1114.8961, -1115.1442, -1115.0250, -1115.0988, -1114.9496, -1115.0028,\n",
      "        -1114.9509, -1115.1157, -1114.8306, -1115.2386, -1114.7554, -1114.7120,\n",
      "        -1115.0868, -1115.1437, -1114.8876, -1115.1963, -1114.9589, -1115.2610,\n",
      "        -1115.1364, -1115.2637, -1115.0505, -1114.9183, -1114.9049, -1114.9431,\n",
      "        -1114.8116, -1114.8008, -1115.2416, -1115.0505, -1115.2878, -1115.1172,\n",
      "        -1115.3512, -1115.0499, -1115.0093, -1115.2111, -1114.7153, -1114.6974,\n",
      "        -1114.9918, -1115.0415, -1114.7992, -1114.9186, -1114.8558, -1115.1331,\n",
      "        -1115.0101, -1114.9410, -1115.0623, -1114.9648, -1115.0262, -1114.9028,\n",
      "        -1115.0961, -1114.8324, -1114.9142, -1115.3046, -1115.0499, -1114.9832,\n",
      "        -1115.2491, -1114.8065, -1114.9828, -1115.0774, -1114.7009, -1115.4751,\n",
      "        -1115.0714, -1115.4708, -1114.8165, -1115.1365, -1115.1105, -1114.9434,\n",
      "        -1114.8148, -1114.9299, -1115.2261, -1114.8496, -1115.1414, -1115.1245,\n",
      "        -1114.8455, -1115.1208, -1115.1276, -1115.2306, -1114.8787, -1115.3470,\n",
      "        -1115.2021, -1114.9019, -1114.9783, -1115.0406, -1114.6975, -1114.9731,\n",
      "        -1114.9966, -1115.1285, -1114.6417, -1115.0538, -1114.8370, -1114.7231,\n",
      "        -1114.7657, -1115.0576, -1115.1166, -1115.0112, -1115.2448, -1114.7402,\n",
      "        -1114.6370, -1115.1799, -1115.1772, -1114.5409, -1114.7089, -1115.1256,\n",
      "        -1114.7296, -1114.8452, -1114.9015, -1114.9264, -1115.1587, -1115.1895,\n",
      "        -1114.8806, -1114.6790, -1114.7700, -1114.5623, -1115.0677, -1115.5159,\n",
      "        -1115.3811, -1115.1012, -1114.9641, -1114.9722, -1114.8885, -1115.1516,\n",
      "        -1115.0212, -1114.8468, -1115.0616, -1115.3296, -1114.9680, -1114.5707,\n",
      "        -1115.0912, -1115.1429, -1115.1128, -1114.9933, -1114.8944, -1115.0703,\n",
      "        -1115.1439, -1114.9390, -1115.3120, -1114.8341, -1115.1721, -1115.3645,\n",
      "        -1115.0759, -1115.0129, -1114.8468, -1114.8059, -1115.0476, -1114.8105,\n",
      "        -1114.8458, -1114.9417, -1114.7930, -1115.0995, -1115.1552, -1115.0688,\n",
      "        -1115.2373, -1115.0156, -1114.9069, -1115.2415, -1114.8416, -1114.6976,\n",
      "        -1114.8202, -1115.0516, -1114.9186, -1115.0438, -1114.8220, -1114.8879,\n",
      "        -1114.8185, -1114.7733, -1115.0339, -1115.1191, -1115.3655, -1114.7031,\n",
      "        -1115.0917, -1114.7903, -1114.4332, -1114.8767, -1114.5707, -1114.9323,\n",
      "        -1115.0106, -1115.2178, -1114.7563, -1114.7397, -1114.9653, -1114.5912,\n",
      "        -1115.4446, -1115.2854, -1115.1986, -1115.1599, -1115.1445, -1115.1013,\n",
      "        -1114.9993, -1114.6580, -1114.6732, -1115.3915, -1114.9523, -1114.4921,\n",
      "        -1114.8842, -1114.6373, -1114.9269, -1114.9628, -1114.6783, -1115.2703,\n",
      "        -1115.4933, -1115.0248, -1115.0547, -1114.9271, -1114.7510, -1114.9672,\n",
      "        -1115.2113, -1114.8756, -1114.6990, -1114.9312, -1114.1846, -1114.5520,\n",
      "        -1114.6641, -1115.2858, -1115.1287, -1115.0098, -1115.2274, -1115.4282,\n",
      "        -1114.7692, -1115.0322, -1114.7471, -1114.8788, -1115.0957, -1114.6954,\n",
      "        -1114.9258, -1115.2058, -1115.1704, -1114.9824, -1115.1254, -1114.7860,\n",
      "        -1114.6974, -1114.6866, -1115.4640, -1115.0654, -1114.9701, -1115.1202,\n",
      "        -1114.7980, -1114.5228, -1114.6501, -1115.1060, -1114.9762, -1114.8386,\n",
      "        -1114.8639, -1115.1913, -1115.0316, -1114.8967, -1115.1459, -1115.0291,\n",
      "        -1115.0444, -1115.1035, -1114.6671, -1114.9618, -1114.8549, -1115.2256,\n",
      "        -1115.3165, -1115.0724, -1115.0529, -1114.3468, -1115.0681, -1115.3204,\n",
      "        -1114.9358, -1114.8523, -1114.8221, -1114.9525, -1114.9407, -1115.0326,\n",
      "        -1114.6672, -1115.0760, -1114.9441, -1115.2284, -1114.8032, -1115.1766,\n",
      "        -1114.7009, -1114.9888, -1114.9680, -1114.8894, -1115.0739, -1115.1559,\n",
      "        -1114.3357, -1114.5797, -1114.9067, -1114.7834, -1114.8684, -1114.8737,\n",
      "        -1114.5160, -1115.1830, -1114.8744, -1115.1644, -1114.7382, -1115.1270,\n",
      "        -1115.2031, -1114.7190, -1114.6630, -1114.8215, -1114.9580, -1114.7910,\n",
      "        -1114.8711, -1114.7405, -1114.6925, -1114.8458, -1114.2915, -1114.8472,\n",
      "        -1115.1915, -1114.8135, -1114.8046, -1114.7721, -1114.6332, -1114.7031,\n",
      "        -1114.9794, -1114.8315, -1115.1418, -1115.0406, -1114.9463, -1114.7656,\n",
      "        -1115.2670, -1114.4937, -1114.8016, -1114.8903, -1114.8506, -1115.0880,\n",
      "        -1114.7982, -1114.8907, -1115.0411, -1114.8005, -1115.3297, -1114.9900,\n",
      "        -1115.0817, -1114.8839, -1114.9493, -1115.2211, -1115.1860, -1114.9369,\n",
      "        -1115.0261, -1114.8217, -1115.1949, -1115.1427, -1115.1506, -1115.3611,\n",
      "        -1115.0864, -1114.9254, -1114.9690, -1115.5208, -1114.7148, -1114.8127,\n",
      "        -1114.9915, -1115.2427, -1114.7010, -1115.0065, -1114.9657, -1115.4891,\n",
      "        -1114.8829, -1114.8785, -1115.0474, -1115.1223, -1115.0337, -1115.2410,\n",
      "        -1114.7714, -1114.6027, -1115.0546, -1115.0596, -1114.7764, -1114.7933,\n",
      "        -1115.1544, -1115.0803, -1115.2139, -1114.5715, -1115.1216, -1114.9653,\n",
      "        -1114.8610, -1114.9049, -1115.1484, -1114.9181, -1115.1284, -1114.7477,\n",
      "        -1114.7977, -1115.0391, -1114.8539, -1114.7544, -1115.3220, -1115.3741,\n",
      "        -1115.0503, -1114.6531, -1115.3333, -1114.7678, -1115.1357, -1115.1865,\n",
      "        -1114.9386, -1114.5597, -1115.1384, -1115.0309, -1114.6348, -1115.1178,\n",
      "        -1115.0416, -1114.8243, -1114.6144, -1115.4127, -1115.0927, -1114.9657,\n",
      "        -1114.8926, -1114.8589], grad_fn=<AddBackward0>)\n",
      "tensor([1242.8160])\n"
     ]
    }
   ],
   "source": [
    "item_no = 6\n",
    "print(model(validation_dataset[item_no][0]))\n",
    "print(validation_dataset[item_no][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interpreter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b9dd547f57d6b4c560bf05e45ad7b013f97da4096e303462092e61764d40047"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

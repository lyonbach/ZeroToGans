{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link To The Video:\n",
    "#### https://www.youtube.com/watch?v=hvLFD4AZzCw&list=PLyMom0n-MBroupZiLfVSZqK5asX8KfoHL&index=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for building and training a neural network using the PyTorch framework\n",
    "import copy                       # For creating deep and shallow copies of Python objects\n",
    "import torch.nn as nn             # Neural network module in PyTorch\n",
    "import torch.nn.functional as F   # Contains various functions for building neural networks\n",
    "import torch                      # Provides functions and classes for working with tensors and neural networks\n",
    "import PIL                        # Provides functions for working with images in Python\n",
    "import numpy as np                # Provides functions for working with numerical arrays in Python\n",
    "\n",
    "# Import the MNIST dataset\n",
    "from torch.utils.data import DataLoader, random_split  # For loading and iterating over datasets in PyTorch, and splitting a dataset into training and validation subsets\n",
    "from torchvision.datasets import MNIST                 # PyTorch dataset class that provides access to the MNIST dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset and split it into training and validation subsets\n",
    "dataset = MNIST(root='../../data/', download=True)\n",
    "train_ds_, validation_ds_ = random_split(dataset, [50000, 10000])\n",
    "\n",
    "# Convert the image and label data to PyTorch tensors and store them in separate lists for the training and validation subsets\n",
    "train_ds = [(torch.tensor(np.asarray(image_data)).to(torch.float32), (torch.tensor(label)).to(torch.int64)) for image_data, label in train_ds_]\n",
    "validation_ds = [(torch.tensor(np.asarray(image_data)).to(torch.float32), (torch.tensor(label)).to(torch.int64)) for image_data, label in validation_ds_]\n",
    "\n",
    "# Create data loaders for iterating over the training and validation subsets in batches\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_ds, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate accuracy\n",
    "def accuracy(outputs, labels):\n",
    "\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "\n",
    "    VALIDATION_STEP_OUTPUT_TEMPLATE = {\"loss\": None, \"accuracy\": None}\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.loss_fn = F.cross_entropy\n",
    "\n",
    "    def forward(self, xb: torch.Tensor):\n",
    "        xb = xb.reshape(-1, 784)  # We request the other dimension to be calculated by pytorch\n",
    "        return self.linear(xb)\n",
    "    \n",
    "    def training_step(self, batch: list[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n",
    "        \n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        return self.loss_fn(outputs, labels)\n",
    "    \n",
    "    def validation_step(self, batch: list[torch.Tensor, torch.Tensor]) -> dict:\n",
    "\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        t_loss = self.loss_fn(outputs, labels)\n",
    "        t_accuracy = accuracy(outputs, labels)\n",
    "        \n",
    "        output_template = copy.copy(MnistModel.VALIDATION_STEP_OUTPUT_TEMPLATE)\n",
    "        output_template[\"loss\"] = t_loss\n",
    "        output_template[\"accuracy\"] = t_accuracy\n",
    "        return output_template\n",
    "    \n",
    "    def validation_epoch_end(self, validation_step_outputs: dict):\n",
    "\n",
    "        batch_losses = [result[\"loss\"] for result in validation_step_outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()  # Combine losses\n",
    "        batch_accuracies = [result[\"accuracy\"] for result in validation_step_outputs]\n",
    "        epoch_accuracy = torch.stack(batch_accuracies).mean()  # Combine accuracies\n",
    "        \n",
    "        output_template = copy.copy(MnistModel.VALIDATION_STEP_OUTPUT_TEMPLATE)\n",
    "        output_template[\"loss\"] = epoch_loss.item()  # Get value using \".item()\"\n",
    "        output_template[\"accuracy\"] = epoch_accuracy.item()\n",
    "        return output_template\n",
    "    \n",
    "    def print_result(self, epoch, result):\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}], loss: {result['loss']:.4f}, accuracy: {result['accuracy']:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate accuracy\n",
    "def evaluate(model, validation_loader):\n",
    "\n",
    "    outputs = [model.validation_step(batch) for batch in validation_loader]\n",
    "    return model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, lr, model, train_loader, validation_loader, optimizer_function=torch.optim.SGD):\n",
    "\n",
    "    optimizer = optimizer_function(model.parameters(), lr)\n",
    "    history = []  # To record what happens during the training.\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Training phase\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()       # Parameter update.\n",
    "            optimizer.zero_grad()  # Reset gradients.\n",
    "\n",
    "        # Validation phase\n",
    "        result = evaluate(model, validation_loader)\n",
    "        model.print_result(epoch, result)\n",
    "        history.append(result)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistModel(28*28, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 95.21365356445312, 'accuracy': 0.0768394023180008}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], loss: 0.7240, accuracy: 0.8909\n",
      "Epoch [2], loss: 0.7189, accuracy: 0.8915\n",
      "Epoch [3], loss: 0.7133, accuracy: 0.8912\n",
      "Epoch [4], loss: 0.7183, accuracy: 0.8923\n",
      "Epoch [5], loss: 0.7175, accuracy: 0.8914\n",
      "Epoch [6], loss: 0.7316, accuracy: 0.8876\n",
      "Epoch [7], loss: 0.7123, accuracy: 0.8896\n",
      "Epoch [8], loss: 0.7044, accuracy: 0.8923\n",
      "Epoch [9], loss: 0.6961, accuracy: 0.8929\n",
      "Epoch [10], loss: 0.6995, accuracy: 0.8940\n",
      "Epoch [11], loss: 0.6968, accuracy: 0.8925\n",
      "Epoch [12], loss: 0.7076, accuracy: 0.8892\n",
      "Epoch [13], loss: 0.6846, accuracy: 0.8927\n",
      "Epoch [14], loss: 0.7056, accuracy: 0.8886\n",
      "Epoch [15], loss: 0.6910, accuracy: 0.8920\n",
      "Epoch [16], loss: 0.6880, accuracy: 0.8920\n",
      "Epoch [17], loss: 0.6843, accuracy: 0.8927\n",
      "Epoch [18], loss: 0.6787, accuracy: 0.8928\n",
      "Epoch [19], loss: 0.6736, accuracy: 0.8935\n",
      "Epoch [20], loss: 0.6847, accuracy: 0.8907\n",
      "Epoch [21], loss: 0.6672, accuracy: 0.8944\n",
      "Epoch [22], loss: 0.6668, accuracy: 0.8924\n",
      "Epoch [23], loss: 0.6873, accuracy: 0.8894\n",
      "Epoch [24], loss: 0.6734, accuracy: 0.8926\n",
      "Epoch [25], loss: 0.6691, accuracy: 0.8916\n",
      "Epoch [26], loss: 0.6721, accuracy: 0.8901\n",
      "Epoch [27], loss: 0.6620, accuracy: 0.8939\n",
      "Epoch [28], loss: 0.6663, accuracy: 0.8912\n",
      "Epoch [29], loss: 0.6508, accuracy: 0.8965\n",
      "Epoch [30], loss: 0.6575, accuracy: 0.8934\n",
      "Epoch [31], loss: 0.6517, accuracy: 0.8947\n",
      "Epoch [32], loss: 0.6472, accuracy: 0.8949\n",
      "Epoch [33], loss: 0.6491, accuracy: 0.8936\n",
      "Epoch [34], loss: 0.6448, accuracy: 0.8922\n",
      "Epoch [35], loss: 0.6489, accuracy: 0.8935\n",
      "Epoch [36], loss: 0.6402, accuracy: 0.8942\n",
      "Epoch [37], loss: 0.6471, accuracy: 0.8925\n",
      "Epoch [38], loss: 0.6325, accuracy: 0.8962\n",
      "Epoch [39], loss: 0.6392, accuracy: 0.8931\n",
      "Epoch [40], loss: 0.6303, accuracy: 0.8949\n",
      "Epoch [41], loss: 0.6342, accuracy: 0.8931\n",
      "Epoch [42], loss: 0.6266, accuracy: 0.8962\n",
      "Epoch [43], loss: 0.6218, accuracy: 0.8949\n",
      "Epoch [44], loss: 0.6324, accuracy: 0.8932\n",
      "Epoch [45], loss: 0.6309, accuracy: 0.8934\n",
      "Epoch [46], loss: 0.6140, accuracy: 0.8969\n",
      "Epoch [47], loss: 0.6260, accuracy: 0.8934\n",
      "Epoch [48], loss: 0.6172, accuracy: 0.8956\n",
      "Epoch [49], loss: 0.6136, accuracy: 0.8940\n",
      "Epoch [50], loss: 0.6319, accuracy: 0.8881\n"
     ]
    }
   ],
   "source": [
    "fit(50, .00001, model, train_loader, validation_loader);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds__, validation_ds__ = random_split(dataset, [50000, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "image = PIL.Image.open(\"/home/lyonbach/Documents/eight_2ie.jpg\")\n",
    "image_as_array = np.asarray(image)[:,:, 1].reshape(28, 28, 1)\n",
    "image_as_tensor = torch.Tensor(image_as_array)\n",
    "results = model(image_as_tensor).tolist()[0]\n",
    "print(results.index(max(results)))\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "image, label = train_ds__[280]\n",
    "\n",
    "image_as_array = np.asarray(image)\n",
    "image_as_tensor = torch.Tensor(image_as_array)\n",
    "results = model(image_as_tensor).tolist()[0]\n",
    "\n",
    "\n",
    "print(results.index(max(results)))\n",
    "\n",
    "image.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interpreter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b9dd547f57d6b4c560bf05e45ad7b013f97da4096e303462092e61764d40047"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

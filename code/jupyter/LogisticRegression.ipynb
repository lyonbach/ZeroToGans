{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link To The Video:\n",
    "#### https://www.youtube.com/watch?v=hvLFD4AZzCw&list=PLyMom0n-MBroupZiLfVSZqK5asX8KfoHL&index=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for building and training a neural network using the PyTorch framework\n",
    "import copy                       # For creating deep and shallow copies of Python objects\n",
    "import torch.nn as nn             # Neural network module in PyTorch\n",
    "import torch.nn.functional as F   # Contains various functions for building neural networks\n",
    "import torch                      # Provides functions and classes for working with tensors and neural networks\n",
    "import PIL                        # Provides functions for working with images in Python\n",
    "import numpy as np                # Provides functions for working with numerical arrays in Python\n",
    "\n",
    "# Import the MNIST dataset\n",
    "from torch.utils.data import DataLoader, random_split  # For loading and iterating over datasets in PyTorch, and splitting a dataset into training and validation subsets\n",
    "from torchvision.datasets import MNIST                 # PyTorch dataset class that provides access to the MNIST dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset and split it into training and validation subsets\n",
    "dataset = MNIST(root='../../data/', download=True)\n",
    "train_ds_, validation_ds_ = random_split(dataset, [50000, 10000])\n",
    "\n",
    "# Convert the image and label data to PyTorch tensors and store them in separate lists for the training and validation subsets\n",
    "train_ds = [(torch.tensor(np.asarray(image_data)).to(torch.float32), (torch.tensor(label)).to(torch.int64)) for image_data, label in train_ds_]\n",
    "validation_ds = [(torch.tensor(np.asarray(image_data)).to(torch.float32), (torch.tensor(label)).to(torch.int64)) for image_data, label in validation_ds_]\n",
    "\n",
    "# Create data loaders for iterating over the training and validation subsets in batches\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_ds, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate accuracy\n",
    "def accuracy(outputs, labels):\n",
    "\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "\n",
    "    VALIDATION_STEP_OUTPUT_TEMPLATE = {\"loss\": None, \"accuracy\": None}\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.loss_fn = F.cross_entropy\n",
    "\n",
    "    def forward(self, xb: torch.Tensor):\n",
    "        xb = xb.reshape(-1, 784)  # We request the other dimension to be calculated by pytorch\n",
    "        return self.linear(xb)\n",
    "    \n",
    "    def training_step(self, batch: list[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n",
    "        \n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        return self.loss_fn(outputs, labels)\n",
    "    \n",
    "    def validation_step(self, batch: list[torch.Tensor, torch.Tensor]) -> dict:\n",
    "\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        t_loss = self.loss_fn(outputs, labels)\n",
    "        t_accuracy = accuracy(outputs, labels)\n",
    "        \n",
    "        output_template = copy.copy(MnistModel.VALIDATION_STEP_OUTPUT_TEMPLATE)\n",
    "        output_template[\"loss\"] = t_loss\n",
    "        output_template[\"accuracy\"] = t_accuracy\n",
    "        return output_template\n",
    "    \n",
    "    def validation_epoch_end(self, validation_step_outputs: dict):\n",
    "\n",
    "        batch_losses = [result[\"loss\"] for result in validation_step_outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()  # Combine losses\n",
    "        batch_accuracies = [result[\"accuracy\"] for result in validation_step_outputs]\n",
    "        epoch_accuracy = torch.stack(batch_accuracies).mean()  # Combine accuracies\n",
    "        \n",
    "        output_template = copy.copy(MnistModel.VALIDATION_STEP_OUTPUT_TEMPLATE)\n",
    "        output_template[\"loss\"] = epoch_loss.item()  # Get value using \".item()\"\n",
    "        output_template[\"accuracy\"] = epoch_accuracy.item()\n",
    "        return output_template\n",
    "    \n",
    "    def print_result(self, epoch, result):\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}], loss: {result['loss']:.4f}, accuracy: {result['accuracy']:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate accuracy\n",
    "def evaluate(model, validation_loader):\n",
    "\n",
    "    outputs = [model.validation_step(batch) for batch in validation_loader]\n",
    "    return model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, lr, model, train_loader, validation_loader, optimizer_function=torch.optim.SGD):\n",
    "\n",
    "    optimizer = optimizer_function(model.parameters(), lr)\n",
    "    history = []  # To record what happens during the training.\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Training phase\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()       # Parameter update.\n",
    "            optimizer.zero_grad()  # Reset gradients.\n",
    "\n",
    "        # Validation phase\n",
    "        result = evaluate(model, validation_loader)\n",
    "        model.print_result(epoch, result)\n",
    "        history.append(result)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistModel(28*28, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 84.28312683105469, 'accuracy': 0.04736946150660515}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], loss: 0.9149, accuracy: 0.8839\n",
      "Epoch [2], loss: 0.9143, accuracy: 0.8835\n",
      "Epoch [3], loss: 0.9150, accuracy: 0.8824\n",
      "Epoch [4], loss: 0.9148, accuracy: 0.8824\n",
      "Epoch [5], loss: 0.9136, accuracy: 0.8836\n",
      "Epoch [6], loss: 0.9124, accuracy: 0.8847\n",
      "Epoch [7], loss: 0.9118, accuracy: 0.8838\n",
      "Epoch [8], loss: 0.9112, accuracy: 0.8837\n",
      "Epoch [9], loss: 0.9104, accuracy: 0.8835\n",
      "Epoch [10], loss: 0.9126, accuracy: 0.8831\n",
      "Epoch [11], loss: 0.9099, accuracy: 0.8834\n",
      "Epoch [12], loss: 0.9093, accuracy: 0.8836\n",
      "Epoch [13], loss: 0.9089, accuracy: 0.8836\n",
      "Epoch [14], loss: 0.9084, accuracy: 0.8830\n",
      "Epoch [15], loss: 0.9082, accuracy: 0.8831\n",
      "Epoch [16], loss: 0.9088, accuracy: 0.8828\n",
      "Epoch [17], loss: 0.9075, accuracy: 0.8828\n",
      "Epoch [18], loss: 0.9081, accuracy: 0.8824\n",
      "Epoch [19], loss: 0.9073, accuracy: 0.8827\n",
      "Epoch [20], loss: 0.9055, accuracy: 0.8843\n",
      "Epoch [21], loss: 0.9067, accuracy: 0.8825\n",
      "Epoch [22], loss: 0.9048, accuracy: 0.8829\n",
      "Epoch [23], loss: 0.9032, accuracy: 0.8831\n",
      "Epoch [24], loss: 0.9042, accuracy: 0.8818\n",
      "Epoch [25], loss: 0.9037, accuracy: 0.8827\n",
      "Epoch [26], loss: 0.9037, accuracy: 0.8834\n",
      "Epoch [27], loss: 0.9007, accuracy: 0.8834\n",
      "Epoch [28], loss: 0.9012, accuracy: 0.8830\n",
      "Epoch [29], loss: 0.9007, accuracy: 0.8834\n",
      "Epoch [30], loss: 0.9011, accuracy: 0.8834\n",
      "Epoch [31], loss: 0.9012, accuracy: 0.8832\n",
      "Epoch [32], loss: 0.9000, accuracy: 0.8829\n",
      "Epoch [33], loss: 0.8984, accuracy: 0.8835\n",
      "Epoch [34], loss: 0.8985, accuracy: 0.8832\n",
      "Epoch [35], loss: 0.8990, accuracy: 0.8832\n",
      "Epoch [36], loss: 0.8981, accuracy: 0.8830\n",
      "Epoch [37], loss: 0.8983, accuracy: 0.8826\n",
      "Epoch [38], loss: 0.8961, accuracy: 0.8835\n",
      "Epoch [39], loss: 0.8963, accuracy: 0.8832\n",
      "Epoch [40], loss: 0.8977, accuracy: 0.8829\n",
      "Epoch [41], loss: 0.8951, accuracy: 0.8830\n",
      "Epoch [42], loss: 0.8941, accuracy: 0.8832\n",
      "Epoch [43], loss: 0.8942, accuracy: 0.8823\n",
      "Epoch [44], loss: 0.8947, accuracy: 0.8826\n",
      "Epoch [45], loss: 0.8938, accuracy: 0.8831\n",
      "Epoch [46], loss: 0.8923, accuracy: 0.8828\n",
      "Epoch [47], loss: 0.8920, accuracy: 0.8830\n",
      "Epoch [48], loss: 0.8927, accuracy: 0.8827\n",
      "Epoch [49], loss: 0.8910, accuracy: 0.8839\n",
      "Epoch [50], loss: 0.8907, accuracy: 0.8835\n",
      "Epoch [51], loss: 0.8915, accuracy: 0.8830\n",
      "Epoch [52], loss: 0.8896, accuracy: 0.8835\n",
      "Epoch [53], loss: 0.8887, accuracy: 0.8838\n",
      "Epoch [54], loss: 0.8900, accuracy: 0.8822\n",
      "Epoch [55], loss: 0.8881, accuracy: 0.8835\n",
      "Epoch [56], loss: 0.8881, accuracy: 0.8824\n",
      "Epoch [57], loss: 0.8877, accuracy: 0.8833\n",
      "Epoch [58], loss: 0.8858, accuracy: 0.8834\n",
      "Epoch [59], loss: 0.8854, accuracy: 0.8836\n",
      "Epoch [60], loss: 0.8872, accuracy: 0.8828\n",
      "Epoch [61], loss: 0.8841, accuracy: 0.8829\n",
      "Epoch [62], loss: 0.8846, accuracy: 0.8830\n",
      "Epoch [63], loss: 0.8835, accuracy: 0.8831\n",
      "Epoch [64], loss: 0.8843, accuracy: 0.8830\n",
      "Epoch [65], loss: 0.8844, accuracy: 0.8827\n",
      "Epoch [66], loss: 0.8843, accuracy: 0.8831\n",
      "Epoch [67], loss: 0.8822, accuracy: 0.8832\n",
      "Epoch [68], loss: 0.8825, accuracy: 0.8826\n",
      "Epoch [69], loss: 0.8805, accuracy: 0.8830\n",
      "Epoch [70], loss: 0.8814, accuracy: 0.8828\n",
      "Epoch [71], loss: 0.8803, accuracy: 0.8827\n",
      "Epoch [72], loss: 0.8798, accuracy: 0.8834\n",
      "Epoch [73], loss: 0.8809, accuracy: 0.8830\n",
      "Epoch [74], loss: 0.8808, accuracy: 0.8820\n",
      "Epoch [75], loss: 0.8783, accuracy: 0.8828\n",
      "Epoch [76], loss: 0.8776, accuracy: 0.8843\n",
      "Epoch [77], loss: 0.8772, accuracy: 0.8826\n",
      "Epoch [78], loss: 0.8763, accuracy: 0.8832\n",
      "Epoch [79], loss: 0.8770, accuracy: 0.8844\n",
      "Epoch [80], loss: 0.8776, accuracy: 0.8829\n",
      "Epoch [81], loss: 0.8764, accuracy: 0.8831\n",
      "Epoch [82], loss: 0.8762, accuracy: 0.8825\n",
      "Epoch [83], loss: 0.8743, accuracy: 0.8835\n",
      "Epoch [84], loss: 0.8743, accuracy: 0.8828\n",
      "Epoch [85], loss: 0.8736, accuracy: 0.8832\n",
      "Epoch [86], loss: 0.8733, accuracy: 0.8840\n",
      "Epoch [87], loss: 0.8734, accuracy: 0.8830\n",
      "Epoch [88], loss: 0.8719, accuracy: 0.8833\n",
      "Epoch [89], loss: 0.8714, accuracy: 0.8831\n",
      "Epoch [90], loss: 0.8723, accuracy: 0.8828\n",
      "Epoch [91], loss: 0.8719, accuracy: 0.8829\n",
      "Epoch [92], loss: 0.8699, accuracy: 0.8834\n",
      "Epoch [93], loss: 0.8706, accuracy: 0.8825\n",
      "Epoch [94], loss: 0.8685, accuracy: 0.8831\n",
      "Epoch [95], loss: 0.8717, accuracy: 0.8826\n",
      "Epoch [96], loss: 0.8691, accuracy: 0.8828\n",
      "Epoch [97], loss: 0.8703, accuracy: 0.8825\n",
      "Epoch [98], loss: 0.8686, accuracy: 0.8831\n",
      "Epoch [99], loss: 0.8668, accuracy: 0.8835\n",
      "Epoch [100], loss: 0.8667, accuracy: 0.8834\n",
      "Epoch [101], loss: 0.8661, accuracy: 0.8834\n",
      "Epoch [102], loss: 0.8667, accuracy: 0.8828\n",
      "Epoch [103], loss: 0.8671, accuracy: 0.8830\n",
      "Epoch [104], loss: 0.8654, accuracy: 0.8827\n",
      "Epoch [105], loss: 0.8636, accuracy: 0.8839\n",
      "Epoch [106], loss: 0.8645, accuracy: 0.8828\n",
      "Epoch [107], loss: 0.8635, accuracy: 0.8825\n",
      "Epoch [108], loss: 0.8631, accuracy: 0.8839\n",
      "Epoch [109], loss: 0.8627, accuracy: 0.8835\n",
      "Epoch [110], loss: 0.8625, accuracy: 0.8836\n",
      "Epoch [111], loss: 0.8616, accuracy: 0.8835\n",
      "Epoch [112], loss: 0.8623, accuracy: 0.8829\n",
      "Epoch [113], loss: 0.8605, accuracy: 0.8838\n",
      "Epoch [114], loss: 0.8606, accuracy: 0.8842\n",
      "Epoch [115], loss: 0.8601, accuracy: 0.8833\n",
      "Epoch [116], loss: 0.8595, accuracy: 0.8830\n",
      "Epoch [117], loss: 0.8608, accuracy: 0.8832\n",
      "Epoch [118], loss: 0.8589, accuracy: 0.8835\n",
      "Epoch [119], loss: 0.8574, accuracy: 0.8844\n",
      "Epoch [120], loss: 0.8568, accuracy: 0.8840\n",
      "Epoch [121], loss: 0.8577, accuracy: 0.8830\n",
      "Epoch [122], loss: 0.8554, accuracy: 0.8839\n",
      "Epoch [123], loss: 0.8579, accuracy: 0.8839\n",
      "Epoch [124], loss: 0.8563, accuracy: 0.8841\n",
      "Epoch [125], loss: 0.8542, accuracy: 0.8839\n",
      "Epoch [126], loss: 0.8537, accuracy: 0.8838\n",
      "Epoch [127], loss: 0.8540, accuracy: 0.8842\n",
      "Epoch [128], loss: 0.8543, accuracy: 0.8833\n",
      "Epoch [129], loss: 0.8529, accuracy: 0.8841\n",
      "Epoch [130], loss: 0.8522, accuracy: 0.8836\n",
      "Epoch [131], loss: 0.8525, accuracy: 0.8836\n",
      "Epoch [132], loss: 0.8526, accuracy: 0.8835\n",
      "Epoch [133], loss: 0.8506, accuracy: 0.8838\n",
      "Epoch [134], loss: 0.8512, accuracy: 0.8844\n",
      "Epoch [135], loss: 0.8517, accuracy: 0.8829\n",
      "Epoch [136], loss: 0.8523, accuracy: 0.8831\n",
      "Epoch [137], loss: 0.8503, accuracy: 0.8844\n",
      "Epoch [138], loss: 0.8503, accuracy: 0.8832\n",
      "Epoch [139], loss: 0.8482, accuracy: 0.8837\n",
      "Epoch [140], loss: 0.8497, accuracy: 0.8830\n",
      "Epoch [141], loss: 0.8488, accuracy: 0.8841\n",
      "Epoch [142], loss: 0.8469, accuracy: 0.8847\n",
      "Epoch [143], loss: 0.8471, accuracy: 0.8843\n",
      "Epoch [144], loss: 0.8472, accuracy: 0.8837\n",
      "Epoch [145], loss: 0.8467, accuracy: 0.8835\n",
      "Epoch [146], loss: 0.8454, accuracy: 0.8836\n",
      "Epoch [147], loss: 0.8450, accuracy: 0.8839\n",
      "Epoch [148], loss: 0.8438, accuracy: 0.8845\n",
      "Epoch [149], loss: 0.8446, accuracy: 0.8842\n",
      "Epoch [150], loss: 0.8447, accuracy: 0.8845\n",
      "Epoch [151], loss: 0.8432, accuracy: 0.8848\n",
      "Epoch [152], loss: 0.8438, accuracy: 0.8837\n",
      "Epoch [153], loss: 0.8440, accuracy: 0.8846\n",
      "Epoch [154], loss: 0.8427, accuracy: 0.8834\n",
      "Epoch [155], loss: 0.8430, accuracy: 0.8836\n",
      "Epoch [156], loss: 0.8415, accuracy: 0.8847\n",
      "Epoch [157], loss: 0.8405, accuracy: 0.8846\n",
      "Epoch [158], loss: 0.8401, accuracy: 0.8846\n",
      "Epoch [159], loss: 0.8410, accuracy: 0.8836\n",
      "Epoch [160], loss: 0.8418, accuracy: 0.8842\n",
      "Epoch [161], loss: 0.8396, accuracy: 0.8841\n",
      "Epoch [162], loss: 0.8406, accuracy: 0.8846\n",
      "Epoch [163], loss: 0.8390, accuracy: 0.8833\n",
      "Epoch [164], loss: 0.8393, accuracy: 0.8844\n",
      "Epoch [165], loss: 0.8388, accuracy: 0.8840\n",
      "Epoch [166], loss: 0.8364, accuracy: 0.8842\n",
      "Epoch [167], loss: 0.8375, accuracy: 0.8839\n",
      "Epoch [168], loss: 0.8376, accuracy: 0.8840\n",
      "Epoch [169], loss: 0.8362, accuracy: 0.8840\n",
      "Epoch [170], loss: 0.8352, accuracy: 0.8847\n",
      "Epoch [171], loss: 0.8356, accuracy: 0.8845\n",
      "Epoch [172], loss: 0.8355, accuracy: 0.8846\n",
      "Epoch [173], loss: 0.8345, accuracy: 0.8854\n",
      "Epoch [174], loss: 0.8348, accuracy: 0.8842\n",
      "Epoch [175], loss: 0.8352, accuracy: 0.8842\n",
      "Epoch [176], loss: 0.8344, accuracy: 0.8843\n",
      "Epoch [177], loss: 0.8333, accuracy: 0.8834\n",
      "Epoch [178], loss: 0.8325, accuracy: 0.8837\n",
      "Epoch [179], loss: 0.8317, accuracy: 0.8846\n",
      "Epoch [180], loss: 0.8302, accuracy: 0.8852\n",
      "Epoch [181], loss: 0.8319, accuracy: 0.8836\n",
      "Epoch [182], loss: 0.8314, accuracy: 0.8848\n",
      "Epoch [183], loss: 0.8315, accuracy: 0.8839\n",
      "Epoch [184], loss: 0.8305, accuracy: 0.8841\n",
      "Epoch [185], loss: 0.8294, accuracy: 0.8833\n",
      "Epoch [186], loss: 0.8298, accuracy: 0.8833\n",
      "Epoch [187], loss: 0.8301, accuracy: 0.8845\n",
      "Epoch [188], loss: 0.8279, accuracy: 0.8852\n",
      "Epoch [189], loss: 0.8280, accuracy: 0.8849\n",
      "Epoch [190], loss: 0.8283, accuracy: 0.8840\n",
      "Epoch [191], loss: 0.8271, accuracy: 0.8848\n",
      "Epoch [192], loss: 0.8268, accuracy: 0.8847\n",
      "Epoch [193], loss: 0.8258, accuracy: 0.8846\n",
      "Epoch [194], loss: 0.8267, accuracy: 0.8839\n",
      "Epoch [195], loss: 0.8252, accuracy: 0.8853\n",
      "Epoch [196], loss: 0.8253, accuracy: 0.8847\n",
      "Epoch [197], loss: 0.8240, accuracy: 0.8843\n",
      "Epoch [198], loss: 0.8247, accuracy: 0.8842\n",
      "Epoch [199], loss: 0.8238, accuracy: 0.8837\n",
      "Epoch [200], loss: 0.8229, accuracy: 0.8852\n",
      "Epoch [201], loss: 0.8222, accuracy: 0.8849\n",
      "Epoch [202], loss: 0.8220, accuracy: 0.8850\n",
      "Epoch [203], loss: 0.8229, accuracy: 0.8851\n",
      "Epoch [204], loss: 0.8216, accuracy: 0.8848\n",
      "Epoch [205], loss: 0.8208, accuracy: 0.8854\n",
      "Epoch [206], loss: 0.8208, accuracy: 0.8847\n",
      "Epoch [207], loss: 0.8201, accuracy: 0.8853\n",
      "Epoch [208], loss: 0.8207, accuracy: 0.8854\n",
      "Epoch [209], loss: 0.8191, accuracy: 0.8853\n",
      "Epoch [210], loss: 0.8216, accuracy: 0.8844\n",
      "Epoch [211], loss: 0.8185, accuracy: 0.8856\n",
      "Epoch [212], loss: 0.8208, accuracy: 0.8848\n",
      "Epoch [213], loss: 0.8185, accuracy: 0.8850\n",
      "Epoch [214], loss: 0.8187, accuracy: 0.8855\n",
      "Epoch [215], loss: 0.8172, accuracy: 0.8855\n",
      "Epoch [216], loss: 0.8175, accuracy: 0.8838\n",
      "Epoch [217], loss: 0.8163, accuracy: 0.8844\n",
      "Epoch [218], loss: 0.8169, accuracy: 0.8851\n",
      "Epoch [219], loss: 0.8174, accuracy: 0.8844\n",
      "Epoch [220], loss: 0.8158, accuracy: 0.8852\n",
      "Epoch [221], loss: 0.8144, accuracy: 0.8852\n",
      "Epoch [222], loss: 0.8150, accuracy: 0.8854\n",
      "Epoch [223], loss: 0.8147, accuracy: 0.8855\n",
      "Epoch [224], loss: 0.8137, accuracy: 0.8852\n",
      "Epoch [225], loss: 0.8124, accuracy: 0.8860\n",
      "Epoch [226], loss: 0.8121, accuracy: 0.8861\n",
      "Epoch [227], loss: 0.8130, accuracy: 0.8853\n",
      "Epoch [228], loss: 0.8133, accuracy: 0.8852\n",
      "Epoch [229], loss: 0.8117, accuracy: 0.8856\n",
      "Epoch [230], loss: 0.8118, accuracy: 0.8851\n",
      "Epoch [231], loss: 0.8102, accuracy: 0.8855\n",
      "Epoch [232], loss: 0.8103, accuracy: 0.8853\n",
      "Epoch [233], loss: 0.8095, accuracy: 0.8861\n",
      "Epoch [234], loss: 0.8095, accuracy: 0.8859\n",
      "Epoch [235], loss: 0.8096, accuracy: 0.8857\n",
      "Epoch [236], loss: 0.8089, accuracy: 0.8850\n",
      "Epoch [237], loss: 0.8090, accuracy: 0.8854\n",
      "Epoch [238], loss: 0.8077, accuracy: 0.8852\n",
      "Epoch [239], loss: 0.8083, accuracy: 0.8853\n",
      "Epoch [240], loss: 0.8075, accuracy: 0.8862\n",
      "Epoch [241], loss: 0.8064, accuracy: 0.8858\n",
      "Epoch [242], loss: 0.8067, accuracy: 0.8851\n",
      "Epoch [243], loss: 0.8067, accuracy: 0.8853\n",
      "Epoch [244], loss: 0.8067, accuracy: 0.8851\n",
      "Epoch [245], loss: 0.8053, accuracy: 0.8848\n",
      "Epoch [246], loss: 0.8050, accuracy: 0.8858\n",
      "Epoch [247], loss: 0.8058, accuracy: 0.8856\n",
      "Epoch [248], loss: 0.8054, accuracy: 0.8849\n",
      "Epoch [249], loss: 0.8045, accuracy: 0.8852\n",
      "Epoch [250], loss: 0.8040, accuracy: 0.8849\n",
      "Epoch [251], loss: 0.8030, accuracy: 0.8858\n",
      "Epoch [252], loss: 0.8026, accuracy: 0.8860\n",
      "Epoch [253], loss: 0.8021, accuracy: 0.8859\n",
      "Epoch [254], loss: 0.8017, accuracy: 0.8853\n",
      "Epoch [255], loss: 0.8019, accuracy: 0.8855\n",
      "Epoch [256], loss: 0.8021, accuracy: 0.8856\n",
      "Epoch [257], loss: 0.8009, accuracy: 0.8865\n",
      "Epoch [258], loss: 0.8005, accuracy: 0.8857\n",
      "Epoch [259], loss: 0.8009, accuracy: 0.8849\n",
      "Epoch [260], loss: 0.7998, accuracy: 0.8858\n",
      "Epoch [261], loss: 0.8004, accuracy: 0.8858\n",
      "Epoch [262], loss: 0.7995, accuracy: 0.8860\n",
      "Epoch [263], loss: 0.7989, accuracy: 0.8854\n",
      "Epoch [264], loss: 0.7990, accuracy: 0.8862\n",
      "Epoch [265], loss: 0.7979, accuracy: 0.8865\n",
      "Epoch [266], loss: 0.7983, accuracy: 0.8858\n",
      "Epoch [267], loss: 0.7977, accuracy: 0.8851\n",
      "Epoch [268], loss: 0.7978, accuracy: 0.8854\n",
      "Epoch [269], loss: 0.7979, accuracy: 0.8859\n",
      "Epoch [270], loss: 0.7971, accuracy: 0.8852\n",
      "Epoch [271], loss: 0.7968, accuracy: 0.8851\n",
      "Epoch [272], loss: 0.7955, accuracy: 0.8858\n",
      "Epoch [273], loss: 0.7970, accuracy: 0.8853\n",
      "Epoch [274], loss: 0.7947, accuracy: 0.8856\n",
      "Epoch [275], loss: 0.7952, accuracy: 0.8849\n",
      "Epoch [276], loss: 0.7934, accuracy: 0.8863\n",
      "Epoch [277], loss: 0.7944, accuracy: 0.8864\n",
      "Epoch [278], loss: 0.7943, accuracy: 0.8850\n",
      "Epoch [279], loss: 0.7936, accuracy: 0.8854\n",
      "Epoch [280], loss: 0.7936, accuracy: 0.8855\n",
      "Epoch [281], loss: 0.7925, accuracy: 0.8849\n",
      "Epoch [282], loss: 0.7915, accuracy: 0.8851\n",
      "Epoch [283], loss: 0.7912, accuracy: 0.8860\n",
      "Epoch [284], loss: 0.7912, accuracy: 0.8852\n",
      "Epoch [285], loss: 0.7907, accuracy: 0.8865\n",
      "Epoch [286], loss: 0.7915, accuracy: 0.8858\n",
      "Epoch [287], loss: 0.7906, accuracy: 0.8854\n",
      "Epoch [288], loss: 0.7885, accuracy: 0.8864\n",
      "Epoch [289], loss: 0.7897, accuracy: 0.8859\n",
      "Epoch [290], loss: 0.7886, accuracy: 0.8867\n",
      "Epoch [291], loss: 0.7894, accuracy: 0.8852\n",
      "Epoch [292], loss: 0.7902, accuracy: 0.8849\n",
      "Epoch [293], loss: 0.7882, accuracy: 0.8861\n",
      "Epoch [294], loss: 0.7883, accuracy: 0.8855\n",
      "Epoch [295], loss: 0.7885, accuracy: 0.8860\n",
      "Epoch [296], loss: 0.7870, accuracy: 0.8864\n",
      "Epoch [297], loss: 0.7864, accuracy: 0.8873\n",
      "Epoch [298], loss: 0.7865, accuracy: 0.8862\n",
      "Epoch [299], loss: 0.7854, accuracy: 0.8868\n",
      "Epoch [300], loss: 0.7858, accuracy: 0.8862\n",
      "Epoch [301], loss: 0.7856, accuracy: 0.8861\n",
      "Epoch [302], loss: 0.7847, accuracy: 0.8856\n",
      "Epoch [303], loss: 0.7843, accuracy: 0.8861\n",
      "Epoch [304], loss: 0.7840, accuracy: 0.8865\n",
      "Epoch [305], loss: 0.7837, accuracy: 0.8855\n",
      "Epoch [306], loss: 0.7835, accuracy: 0.8865\n",
      "Epoch [307], loss: 0.7831, accuracy: 0.8866\n",
      "Epoch [308], loss: 0.7834, accuracy: 0.8860\n",
      "Epoch [309], loss: 0.7830, accuracy: 0.8865\n",
      "Epoch [310], loss: 0.7819, accuracy: 0.8867\n",
      "Epoch [311], loss: 0.7821, accuracy: 0.8866\n",
      "Epoch [312], loss: 0.7826, accuracy: 0.8863\n",
      "Epoch [313], loss: 0.7810, accuracy: 0.8863\n",
      "Epoch [314], loss: 0.7815, accuracy: 0.8858\n",
      "Epoch [315], loss: 0.7807, accuracy: 0.8863\n",
      "Epoch [316], loss: 0.7810, accuracy: 0.8860\n",
      "Epoch [317], loss: 0.7796, accuracy: 0.8867\n",
      "Epoch [318], loss: 0.7792, accuracy: 0.8862\n",
      "Epoch [319], loss: 0.7787, accuracy: 0.8872\n",
      "Epoch [320], loss: 0.7777, accuracy: 0.8869\n",
      "Epoch [321], loss: 0.7799, accuracy: 0.8865\n",
      "Epoch [322], loss: 0.7776, accuracy: 0.8862\n",
      "Epoch [323], loss: 0.7777, accuracy: 0.8861\n",
      "Epoch [324], loss: 0.7774, accuracy: 0.8863\n",
      "Epoch [325], loss: 0.7771, accuracy: 0.8859\n",
      "Epoch [326], loss: 0.7776, accuracy: 0.8856\n",
      "Epoch [327], loss: 0.7762, accuracy: 0.8862\n",
      "Epoch [328], loss: 0.7759, accuracy: 0.8860\n",
      "Epoch [329], loss: 0.7759, accuracy: 0.8868\n",
      "Epoch [330], loss: 0.7765, accuracy: 0.8859\n",
      "Epoch [331], loss: 0.7747, accuracy: 0.8863\n",
      "Epoch [332], loss: 0.7753, accuracy: 0.8855\n",
      "Epoch [333], loss: 0.7746, accuracy: 0.8863\n",
      "Epoch [334], loss: 0.7741, accuracy: 0.8865\n",
      "Epoch [335], loss: 0.7745, accuracy: 0.8860\n",
      "Epoch [336], loss: 0.7738, accuracy: 0.8864\n",
      "Epoch [337], loss: 0.7731, accuracy: 0.8856\n",
      "Epoch [338], loss: 0.7721, accuracy: 0.8867\n",
      "Epoch [339], loss: 0.7733, accuracy: 0.8864\n",
      "Epoch [340], loss: 0.7721, accuracy: 0.8869\n",
      "Epoch [341], loss: 0.7717, accuracy: 0.8865\n",
      "Epoch [342], loss: 0.7720, accuracy: 0.8865\n",
      "Epoch [343], loss: 0.7708, accuracy: 0.8859\n",
      "Epoch [344], loss: 0.7706, accuracy: 0.8863\n",
      "Epoch [345], loss: 0.7704, accuracy: 0.8864\n",
      "Epoch [346], loss: 0.7691, accuracy: 0.8870\n",
      "Epoch [347], loss: 0.7700, accuracy: 0.8862\n",
      "Epoch [348], loss: 0.7702, accuracy: 0.8865\n",
      "Epoch [349], loss: 0.7693, accuracy: 0.8865\n",
      "Epoch [350], loss: 0.7688, accuracy: 0.8861\n",
      "Epoch [351], loss: 0.7679, accuracy: 0.8862\n",
      "Epoch [352], loss: 0.7690, accuracy: 0.8859\n",
      "Epoch [353], loss: 0.7678, accuracy: 0.8867\n",
      "Epoch [354], loss: 0.7680, accuracy: 0.8867\n",
      "Epoch [355], loss: 0.7676, accuracy: 0.8865\n",
      "Epoch [356], loss: 0.7668, accuracy: 0.8866\n",
      "Epoch [357], loss: 0.7660, accuracy: 0.8867\n",
      "Epoch [358], loss: 0.7672, accuracy: 0.8863\n",
      "Epoch [359], loss: 0.7652, accuracy: 0.8871\n",
      "Epoch [360], loss: 0.7662, accuracy: 0.8860\n",
      "Epoch [361], loss: 0.7647, accuracy: 0.8870\n",
      "Epoch [362], loss: 0.7640, accuracy: 0.8868\n",
      "Epoch [363], loss: 0.7641, accuracy: 0.8867\n",
      "Epoch [364], loss: 0.7646, accuracy: 0.8866\n",
      "Epoch [365], loss: 0.7635, accuracy: 0.8861\n",
      "Epoch [366], loss: 0.7637, accuracy: 0.8867\n",
      "Epoch [367], loss: 0.7643, accuracy: 0.8858\n",
      "Epoch [368], loss: 0.7620, accuracy: 0.8866\n",
      "Epoch [369], loss: 0.7628, accuracy: 0.8867\n",
      "Epoch [370], loss: 0.7623, accuracy: 0.8863\n",
      "Epoch [371], loss: 0.7615, accuracy: 0.8866\n",
      "Epoch [372], loss: 0.7609, accuracy: 0.8869\n",
      "Epoch [373], loss: 0.7600, accuracy: 0.8869\n",
      "Epoch [374], loss: 0.7607, accuracy: 0.8873\n",
      "Epoch [375], loss: 0.7590, accuracy: 0.8878\n",
      "Epoch [376], loss: 0.7592, accuracy: 0.8872\n",
      "Epoch [377], loss: 0.7598, accuracy: 0.8866\n",
      "Epoch [378], loss: 0.7604, accuracy: 0.8864\n",
      "Epoch [379], loss: 0.7582, accuracy: 0.8877\n",
      "Epoch [380], loss: 0.7587, accuracy: 0.8869\n",
      "Epoch [381], loss: 0.7586, accuracy: 0.8864\n",
      "Epoch [382], loss: 0.7583, accuracy: 0.8868\n",
      "Epoch [383], loss: 0.7578, accuracy: 0.8871\n",
      "Epoch [384], loss: 0.7575, accuracy: 0.8868\n",
      "Epoch [385], loss: 0.7582, accuracy: 0.8865\n",
      "Epoch [386], loss: 0.7573, accuracy: 0.8871\n",
      "Epoch [387], loss: 0.7566, accuracy: 0.8869\n",
      "Epoch [388], loss: 0.7564, accuracy: 0.8864\n",
      "Epoch [389], loss: 0.7552, accuracy: 0.8869\n",
      "Epoch [390], loss: 0.7566, accuracy: 0.8865\n",
      "Epoch [391], loss: 0.7556, accuracy: 0.8861\n",
      "Epoch [392], loss: 0.7548, accuracy: 0.8873\n",
      "Epoch [393], loss: 0.7558, accuracy: 0.8868\n",
      "Epoch [394], loss: 0.7543, accuracy: 0.8866\n",
      "Epoch [395], loss: 0.7533, accuracy: 0.8871\n",
      "Epoch [396], loss: 0.7540, accuracy: 0.8869\n",
      "Epoch [397], loss: 0.7528, accuracy: 0.8873\n",
      "Epoch [398], loss: 0.7524, accuracy: 0.8865\n",
      "Epoch [399], loss: 0.7526, accuracy: 0.8876\n",
      "Epoch [400], loss: 0.7523, accuracy: 0.8870\n",
      "Epoch [401], loss: 0.7526, accuracy: 0.8871\n",
      "Epoch [402], loss: 0.7528, accuracy: 0.8865\n",
      "Epoch [403], loss: 0.7528, accuracy: 0.8860\n",
      "Epoch [404], loss: 0.7517, accuracy: 0.8864\n",
      "Epoch [405], loss: 0.7510, accuracy: 0.8868\n",
      "Epoch [406], loss: 0.7504, accuracy: 0.8866\n",
      "Epoch [407], loss: 0.7500, accuracy: 0.8872\n",
      "Epoch [408], loss: 0.7507, accuracy: 0.8868\n",
      "Epoch [409], loss: 0.7504, accuracy: 0.8868\n",
      "Epoch [410], loss: 0.7492, accuracy: 0.8870\n",
      "Epoch [411], loss: 0.7490, accuracy: 0.8871\n",
      "Epoch [412], loss: 0.7482, accuracy: 0.8871\n",
      "Epoch [413], loss: 0.7483, accuracy: 0.8869\n",
      "Epoch [414], loss: 0.7489, accuracy: 0.8863\n",
      "Epoch [415], loss: 0.7474, accuracy: 0.8876\n",
      "Epoch [416], loss: 0.7484, accuracy: 0.8862\n",
      "Epoch [417], loss: 0.7481, accuracy: 0.8871\n",
      "Epoch [418], loss: 0.7472, accuracy: 0.8865\n",
      "Epoch [419], loss: 0.7468, accuracy: 0.8866\n",
      "Epoch [420], loss: 0.7457, accuracy: 0.8871\n",
      "Epoch [421], loss: 0.7461, accuracy: 0.8874\n",
      "Epoch [422], loss: 0.7458, accuracy: 0.8869\n",
      "Epoch [423], loss: 0.7452, accuracy: 0.8872\n",
      "Epoch [424], loss: 0.7451, accuracy: 0.8867\n",
      "Epoch [425], loss: 0.7449, accuracy: 0.8868\n",
      "Epoch [426], loss: 0.7446, accuracy: 0.8872\n",
      "Epoch [427], loss: 0.7441, accuracy: 0.8874\n",
      "Epoch [428], loss: 0.7441, accuracy: 0.8875\n",
      "Epoch [429], loss: 0.7439, accuracy: 0.8868\n",
      "Epoch [430], loss: 0.7434, accuracy: 0.8871\n",
      "Epoch [431], loss: 0.7428, accuracy: 0.8874\n",
      "Epoch [432], loss: 0.7430, accuracy: 0.8870\n",
      "Epoch [433], loss: 0.7425, accuracy: 0.8868\n",
      "Epoch [434], loss: 0.7434, accuracy: 0.8863\n",
      "Epoch [435], loss: 0.7414, accuracy: 0.8870\n",
      "Epoch [436], loss: 0.7418, accuracy: 0.8873\n",
      "Epoch [437], loss: 0.7430, accuracy: 0.8863\n",
      "Epoch [438], loss: 0.7425, accuracy: 0.8864\n",
      "Epoch [439], loss: 0.7415, accuracy: 0.8868\n",
      "Epoch [440], loss: 0.7406, accuracy: 0.8872\n",
      "Epoch [441], loss: 0.7398, accuracy: 0.8873\n",
      "Epoch [442], loss: 0.7402, accuracy: 0.8872\n",
      "Epoch [443], loss: 0.7397, accuracy: 0.8871\n",
      "Epoch [444], loss: 0.7406, accuracy: 0.8869\n",
      "Epoch [445], loss: 0.7387, accuracy: 0.8870\n",
      "Epoch [446], loss: 0.7383, accuracy: 0.8871\n",
      "Epoch [447], loss: 0.7380, accuracy: 0.8870\n",
      "Epoch [448], loss: 0.7393, accuracy: 0.8870\n",
      "Epoch [449], loss: 0.7378, accuracy: 0.8864\n",
      "Epoch [450], loss: 0.7373, accuracy: 0.8866\n",
      "Epoch [451], loss: 0.7388, accuracy: 0.8866\n",
      "Epoch [452], loss: 0.7360, accuracy: 0.8877\n",
      "Epoch [453], loss: 0.7364, accuracy: 0.8875\n",
      "Epoch [454], loss: 0.7367, accuracy: 0.8869\n",
      "Epoch [455], loss: 0.7364, accuracy: 0.8870\n",
      "Epoch [456], loss: 0.7362, accuracy: 0.8872\n",
      "Epoch [457], loss: 0.7363, accuracy: 0.8869\n",
      "Epoch [458], loss: 0.7352, accuracy: 0.8874\n",
      "Epoch [459], loss: 0.7354, accuracy: 0.8865\n",
      "Epoch [460], loss: 0.7359, accuracy: 0.8862\n",
      "Epoch [461], loss: 0.7340, accuracy: 0.8865\n",
      "Epoch [462], loss: 0.7338, accuracy: 0.8871\n",
      "Epoch [463], loss: 0.7338, accuracy: 0.8872\n",
      "Epoch [464], loss: 0.7334, accuracy: 0.8866\n",
      "Epoch [465], loss: 0.7332, accuracy: 0.8867\n",
      "Epoch [466], loss: 0.7352, accuracy: 0.8868\n",
      "Epoch [467], loss: 0.7323, accuracy: 0.8869\n",
      "Epoch [468], loss: 0.7320, accuracy: 0.8868\n",
      "Epoch [469], loss: 0.7328, accuracy: 0.8868\n",
      "Epoch [470], loss: 0.7319, accuracy: 0.8864\n",
      "Epoch [471], loss: 0.7310, accuracy: 0.8872\n",
      "Epoch [472], loss: 0.7316, accuracy: 0.8863\n",
      "Epoch [473], loss: 0.7314, accuracy: 0.8871\n",
      "Epoch [474], loss: 0.7305, accuracy: 0.8865\n",
      "Epoch [475], loss: 0.7306, accuracy: 0.8871\n",
      "Epoch [476], loss: 0.7293, accuracy: 0.8868\n",
      "Epoch [477], loss: 0.7298, accuracy: 0.8866\n",
      "Epoch [478], loss: 0.7295, accuracy: 0.8873\n",
      "Epoch [479], loss: 0.7289, accuracy: 0.8874\n",
      "Epoch [480], loss: 0.7297, accuracy: 0.8868\n",
      "Epoch [481], loss: 0.7287, accuracy: 0.8871\n",
      "Epoch [482], loss: 0.7280, accuracy: 0.8867\n",
      "Epoch [483], loss: 0.7287, accuracy: 0.8864\n",
      "Epoch [484], loss: 0.7277, accuracy: 0.8872\n",
      "Epoch [485], loss: 0.7278, accuracy: 0.8866\n",
      "Epoch [486], loss: 0.7269, accuracy: 0.8869\n",
      "Epoch [487], loss: 0.7272, accuracy: 0.8872\n",
      "Epoch [488], loss: 0.7268, accuracy: 0.8873\n",
      "Epoch [489], loss: 0.7263, accuracy: 0.8865\n",
      "Epoch [490], loss: 0.7265, accuracy: 0.8869\n",
      "Epoch [491], loss: 0.7266, accuracy: 0.8872\n",
      "Epoch [492], loss: 0.7255, accuracy: 0.8874\n",
      "Epoch [493], loss: 0.7256, accuracy: 0.8868\n",
      "Epoch [494], loss: 0.7245, accuracy: 0.8875\n",
      "Epoch [495], loss: 0.7251, accuracy: 0.8866\n",
      "Epoch [496], loss: 0.7241, accuracy: 0.8874\n",
      "Epoch [497], loss: 0.7245, accuracy: 0.8869\n",
      "Epoch [498], loss: 0.7235, accuracy: 0.8870\n",
      "Epoch [499], loss: 0.7250, accuracy: 0.8874\n",
      "Epoch [500], loss: 0.7231, accuracy: 0.8871\n"
     ]
    }
   ],
   "source": [
    "fit(500, .000001, model, train_loader, validation_loader);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image, model):\n",
    "\n",
    "    xb = image.unsqueeze(0)\n",
    "    yb = model(xb)\n",
    "    _, predictions = torch.max(yb, dim=1)\n",
    "    return predictions[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the MNIST dataset for testing\n",
    "# dataset = MNIST(root='../../data/', download=True, train=False, transform=transforms.ToTensor())\n",
    "\n",
    "image_ = PIL.Image.open(\"/home/lyonbach/Documents/seven.jpg\")\n",
    "\n",
    "# image, label = torch.tensor(np.asarray(image_, dtype=float)[:, :, 1]/255).to(torch.float32), 8\n",
    "image, label = dataset[2008]\n",
    "\n",
    "plt.imshow(image[0], cmap=\"gray\")\n",
    "prediction = predict(image, model)\n",
    "print(f\"Prediction: {prediction}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving The Model State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[ 0.0134, -0.0132,  0.0112,  ...,  0.0331,  0.0217,  0.0257],\n",
       "                      [ 0.0095, -0.0005,  0.0298,  ...,  0.0339, -0.0116,  0.0052],\n",
       "                      [ 0.0093,  0.0126, -0.0224,  ...,  0.0193, -0.0173, -0.0297],\n",
       "                      ...,\n",
       "                      [ 0.0097, -0.0178, -0.0014,  ...,  0.0160, -0.0349, -0.0164],\n",
       "                      [ 0.0250, -0.0054, -0.0213,  ...,  0.0175, -0.0281,  0.0219],\n",
       "                      [-0.0300,  0.0346,  0.0228,  ..., -0.0124, -0.0174, -0.0221]])),\n",
       "             ('linear.bias',\n",
       "              tensor([ 0.0312,  0.0274, -0.0111,  0.0353, -0.0122,  0.0172,  0.0310,  0.0199,\n",
       "                      -0.0151, -0.0347]))])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interpreter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b9dd547f57d6b4c560bf05e45ad7b013f97da4096e303462092e61764d40047"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
